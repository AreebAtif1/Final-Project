---
title: "Model"
echo: false
message: false
warning: false
label: setup
---

Using a Bayesian regression model, and  the  formula $$Risk∼β0​+β1​×BMI$$

```{r}
#| label: setup
#| message: FALSE
#| echo: TRUE

# Load necessary libraries
library(tidyverse)
library(brms)

# Load the dataset
data <- read.csv("diabetes.csv")

# Convert 'Outcome' to a factor (0 = No diabetes, 1 = Diabetes)
data$Outcome <- as.factor(data$Outcome)

# Define the Bayesian logistic regression model
model <- brm(
  Outcome ~ BMI,                 # Formula: predicting 'Outcome' based on 'BMI'
  data = data,                   # Dataset
  family = bernoulli(),          # Logistic regression for binary outcome
  prior = c(
    set_prior("normal(0, 10)", class = "b"),     # Prior for the coefficient (BMI)
    set_prior("normal(0, 10)", class = "Intercept")  # Prior for the intercept
  ),
  chains = 4,                    # Number of MCMC chains
  iter = 2000,                   # Number of iterations per chain
  warmup = 1000,                 # Number of warmup (burn-in) iterations
  cores = 2                      # Number of CPU cores to use
)

# Summary of the model
summary(model)

# Plot the posterior distributions and other diagnostics
plot(model)

# Posterior predictive checks
pp_check(model)

```

Certainly! Let’s break down the Bayesian logistic regression formula and its components:

### 1. **Model Formula**

#### Logistic Regression Model:
\[ \text{logit}(p) = \beta_0 + \beta_1 \times \text{BMI} \]

Where:
- **\( p \)** is the probability of the outcome (e.g., having diabetes).
- **\( \text{logit}(p) \)** is the log-odds of the probability \( p \). The logit function is defined as:
  \[ \text{logit}(p) = \log\left(\frac{p}{1 - p}\right) \]
  This transformation converts probabilities (which are between 0 and 1) into a scale where the values can range from \(-\infty\) to \(+\infty\).

- **\( \beta_0 \)** is the intercept of the model. It represents the log-odds of the outcome when all predictors are zero.
- **\( \beta_1 \)** is the coefficient for the predictor variable (in this case, BMI). It measures how the log-odds of the outcome change with a one-unit change in BMI.

### 2. **Likelihood Function**

#### Binary Outcome:
\[ Y \mid \beta_0, \beta_1, \text{BMI} \sim \text{Bernoulli}(p) \]

Where:
- **\( Y \)** is the binary outcome variable (0 or 1).
- **\( p \)** is the probability of \( Y = 1 \) given BMI, which is modeled using the logistic function:
  \[ p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 \times \text{BMI})}} \]
  This is the inverse of the logit function and maps the log-odds back to a probability between 0 and 1.

### 3. **Priors**

In Bayesian regression, you specify prior distributions for the model parameters before observing the data:

- **For the intercept (\( \beta_0 \))**: 
  \[ \beta_0 \sim \text{Normal}(0, 10) \]
  This prior assumes that the intercept follows a normal distribution with mean 0 and standard deviation 10. This is a wide prior that allows a large range of possible intercept values.

- **For the coefficient (\( \beta_1 \))**: 
  \[ \beta_1 \sim \text{Normal}(0, 10) \]
  Similarly, this prior assumes that the coefficient for BMI follows a normal distribution with mean 0 and standard deviation 10.

### Summary

- **Logit Function**: Transforms probability \( p \) to log-odds.
- **Logistic Function**: Converts log-odds back to a probability.
- **Likelihood**: Describes how the observed data \( Y \) is distributed given the parameters and predictor.
- **Priors**: Represent your beliefs about the parameters before seeing the data.

The Bayesian logistic regression model estimates the parameters \( \beta_0 \) and \( \beta_1 \) while incorporating prior beliefs about their distributions and updating these beliefs based on the observed data.
